<p>One alternative is to count the numbers of each character in each string and compare the counts.  A simple implementation should take <code>O(max(N, A))</code> time where N is the length of the larger of the strings, and A is the size of the array you use to store counts.  For example, in Java:</p>

<pre><code>public boolean equalIgnoringOrder(String s1, String s2) {
    if (s1.length() != s2.length()) {
        return false;
    }
    // Assuming characters in the range ASCII 0 to 127 
    int[] c1 = new int[128];
    int[] c2 = new int[128];
    for (int i = 0; i &lt; s1.length(); i++) {
        c1[s1.charAt(i)]++;
        c2[s2.charAt(i)]++;
    }
    for (int i = 0; i &lt; c1.length; i++) {
        if (c1[i] != c2[i]) {
            return false;
        }
    }
    return true;
}
</code></pre>

<p>There are some possible improvements to this.  For example, you can cope with an arbitrary character set by doing a range reduction; i.e. do an initial pass through <code>s1</code> and <code>s2</code> looking for the smallest and largest characters in each one, and use this to determine the size of <code>c1</code> and <code>c2</code> and a base offset.  This will use less space on average and reduce the time to initialize the count arrays.  It also offers a short circuit for the comparison; e.g. when the smallest and largest characters for <code>s1</code> and <code>s2</code> are not the same.</p>

<p>By comparison, comparing strings sorted using heapsort or quicksort would be <code>O(NlogN)</code> on average with <code>O(N)</code> space, where N is the larger string's length.  </p>

<p>However, as @pst points out, the constants of proportionality can make an <code>O(NlogN)</code> or even <code>O(N*N)</code> algorithm better than an <code>O(N)</code> algorithm if N is not large.  In this case, the average lengths of the strings being compared is probably the most important factor.</p>

<p>The code above is effectively performing a Radix Sort with a couple of short circuits.  (Three if you include the short circuit associated with range reduction.)  So ultimately it boils down to whether a Quick Sort / Heap Sort or a Radix Sort would be better.  And that depends on input string lengths and character ranges.</p>

<p><hr /></p>

<p>On a different tack.  @John's answer proposes that we compute a product of prime numbers.  If we do the computation using an arbitrary precision representation, the resulting values will be unique for each distinct set of "equal ignoring order" strings.  Unfortunately, the computation will be <code>O(N*N)</code>.  (Each intermediate product has <code>O(N)</code> digits, and multiplying an N-digit number by a constant is <code>O(N)</code>.  Do that for N characters and you get <code>O(N*N)</code>.)</p>

<p>But if we do the computation modulo (say) 64, the result is effectively a really good hash that is insensitive to character order; e.g.</p>

<pre><code>long hash = 1;
for (int i = 0; i &lt; s.length(); i++) {
    hash = hash * primes[s.charAt(i)];
}
</code></pre>

<p>So, I would claim that the algorithm that gives best performance and space usage <em>on average</em> for comparing randomly generated strings is likely to be of the form:</p>

<pre><code>if (s1.length() != s2.length()) {
    return false;
}
if (hash(s1) != hash(s2)) { // computed as above
    return false;
}
// Compare using sorting or character counting as above.
</code></pre>

<p><hr /></p>

<p>One final point.  If we assume that the string pointers are not identical and that the strings have unequal lengths, any algorithm that computes this <code>equals</code> predicate <strong>has to be</strong> at <code>O(N)</code> or worse.  It has to examine every character in both strings to make this determination, and that takes <code>O(N)</code> operations.  </p>

<p>Any algorithm that does less than <code>2 * N</code> fetches or less than <code>2 * N</code> further operations on the fetched values <em>in this scenario</em> is provably incorrect.</p>
